{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langchain-google-genai python-dotenv"
      ],
      "metadata": {
        "id": "1eHhlwiCtFow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, Any, List\n",
        "from dataclasses import dataclass\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "import json\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"Use Your API Key Here\""
      ],
      "metadata": {
        "id": "3dlYYezZuQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AgentState:\n",
        "    \"\"\"State shared across all nodes in the graph\"\"\"\n",
        "    query: str = \"\"\n",
        "    context: str = \"\"\n",
        "    analysis: str = \"\"\n",
        "    response: str = \"\"\n",
        "    next_action: str = \"\"\n",
        "    iteration: int = 0\n",
        "    max_iterations: int = 3"
      ],
      "metadata": {
        "id": "ojkUNYKNuYdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAIAgent:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        if api_key:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.7,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "        self.analyzer = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.3,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "        self.graph = self._build_graph()\n",
        "\n",
        "    def _build_graph(self) -> StateGraph:\n",
        "        \"\"\"Build the LangGraph workflow\"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        workflow.add_node(\"router\", self._router_node)\n",
        "        workflow.add_node(\"analyzer\", self._analyzer_node)\n",
        "        workflow.add_node(\"researcher\", self._researcher_node)\n",
        "        workflow.add_node(\"responder\", self._responder_node)\n",
        "        workflow.add_node(\"validator\", self._validator_node)\n",
        "\n",
        "        workflow.set_entry_point(\"router\")\n",
        "        workflow.add_edge(\"router\", \"analyzer\")\n",
        "        workflow.add_conditional_edges(\n",
        "            \"analyzer\",\n",
        "            self._decide_next_step,\n",
        "            {\n",
        "                \"research\": \"researcher\",\n",
        "                \"respond\": \"responder\"\n",
        "            }\n",
        "        )\n",
        "        workflow.add_edge(\"researcher\", \"responder\")\n",
        "        workflow.add_edge(\"responder\", \"validator\")\n",
        "        workflow.add_conditional_edges(\n",
        "            \"validator\",\n",
        "            self._should_continue,\n",
        "            {\n",
        "                \"continue\": \"analyzer\",\n",
        "                \"end\": END\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def _router_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Route and categorize the incoming query\"\"\"\n",
        "        system_msg = \"\"\"You are a query router. Analyze the user's query and provide context.\n",
        "        Determine if this is a factual question, creative request, problem-solving task, or analysis.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"Query: {state.query}\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "\n",
        "        return {\n",
        "            \"context\": response.content,\n",
        "            \"iteration\": state.iteration + 1\n",
        "        }\n",
        "\n",
        "    def _analyzer_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze the query and determine the approach\"\"\"\n",
        "        system_msg = \"\"\"Analyze the query and context. Determine if additional research is needed\n",
        "        or if you can provide a direct response. Be thorough in your analysis.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Context: {state.context}\n",
        "            Previous Analysis: {state.analysis}\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.analyzer.invoke(messages)\n",
        "        analysis = response.content\n",
        "\n",
        "        if \"research\" in analysis.lower() or \"more information\" in analysis.lower():\n",
        "            next_action = \"research\"\n",
        "        else:\n",
        "            next_action = \"respond\"\n",
        "\n",
        "        return {\n",
        "            \"analysis\": analysis,\n",
        "            \"next_action\": next_action\n",
        "        }\n",
        "\n",
        "    def _researcher_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Conduct additional research or information gathering\"\"\"\n",
        "        system_msg = \"\"\"You are a research assistant. Based on the analysis, gather relevant\n",
        "        information and insights to help answer the query comprehensively.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Analysis: {state.analysis}\n",
        "            Research focus: Provide detailed information relevant to the query.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "\n",
        "        updated_context = f\"{state.context}\\n\\nResearch: {response.content}\"\n",
        "\n",
        "        return {\"context\": updated_context}\n",
        "\n",
        "    def _responder_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Generate the final response\"\"\"\n",
        "        system_msg = \"\"\"You are a helpful AI assistant. Provide a comprehensive, accurate,\n",
        "        and well-structured response based on the analysis and context provided.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Context: {state.context}\n",
        "            Analysis: {state.analysis}\n",
        "\n",
        "            Provide a complete and helpful response.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "\n",
        "        return {\"response\": response.content}\n",
        "\n",
        "    def _validator_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Validate the response quality and completeness\"\"\"\n",
        "        system_msg = \"\"\"Evaluate if the response adequately answers the query.\n",
        "        Return 'COMPLETE' if satisfactory, or 'NEEDS_IMPROVEMENT' if more work is needed.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Original Query: {state.query}\n",
        "            Response: {state.response}\n",
        "\n",
        "            Is this response complete and satisfactory?\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.analyzer.invoke(messages)\n",
        "        validation = response.content\n",
        "\n",
        "        return {\"context\": f\"{state.context}\\n\\nValidation: {validation}\"}\n",
        "\n",
        "    def _decide_next_step(self, state: AgentState) -> str:\n",
        "        \"\"\"Decide whether to research or respond directly\"\"\"\n",
        "        return state.next_action\n",
        "\n",
        "    def _should_continue(self, state: AgentState) -> str:\n",
        "        \"\"\"Decide whether to continue iterating or end\"\"\"\n",
        "        if state.iteration >= state.max_iterations:\n",
        "            return \"end\"\n",
        "        if \"COMPLETE\" in state.context:\n",
        "            return \"end\"\n",
        "        if \"NEEDS_IMPROVEMENT\" in state.context:\n",
        "            return \"continue\"\n",
        "        return \"end\"\n",
        "\n",
        "    def run(self, query: str) -> str:\n",
        "        \"\"\"Run the agent with a query\"\"\"\n",
        "        initial_state = AgentState(query=query)\n",
        "        result = self.graph.invoke(initial_state)\n",
        "        return result[\"response\"]"
      ],
      "metadata": {
        "id": "7X0-tF14udBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRX-IX3pslwn",
        "outputId": "7b3a57b1-7beb-40db-a82b-39fdf896905b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Graph AI Agent with LangGraph and Gemini\n",
            "==================================================\n",
            "\n",
            "üìù Query 1: Explain quantum computing and its applications\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Response: Quantum computing is a revolutionary field leveraging the principles of quantum mechanics to perform calculations in ways impossible for classical computers.  Unlike classical computers that store information as bits representing either 0 or 1, quantum computers use qubits.  This seemingly small difference unlocks immense computational power.\n",
            "\n",
            "**Fundamental Principles:**\n",
            "\n",
            "Three core quantum phenomena underpin quantum computing's power:\n",
            "\n",
            "* **Superposition:** A qubit can exist in a superposition, simultaneously representing 0 and 1 with certain probabilities.  Imagine a coin spinning in the air ‚Äì it's neither heads nor tails until it lands.  This allows a quantum computer to explore many possibilities concurrently, unlike a classical computer that can only explore one at a time.\n",
            "\n",
            "* **Entanglement:**  Multiple qubits can be entangled, meaning their fates are intertwined. Measuring the state of one instantly reveals the state of the others, regardless of the distance separating them. This creates powerful correlations, enabling complex computations impossible classically.\n",
            "\n",
            "* **Quantum Interference:**  Similar to wave interference, quantum interference allows for constructive and destructive interference of quantum probabilities.  This is crucial for quantum algorithms, amplifying correct solutions and suppressing incorrect ones, leading to significant speedups.\n",
            "\n",
            "\n",
            "**Quantum Computing Architectures:**\n",
            "\n",
            "Several approaches are being developed to build quantum computers, each with its pros and cons:\n",
            "\n",
            "* **Superconducting qubits:**  These use superconducting circuits cooled to near absolute zero.  Google and IBM are leaders in this technology, which offers relatively high coherence times (how long a qubit maintains its quantum state) and scalability potential, but requires expensive cryogenic cooling.\n",
            "\n",
            "* **Trapped ions:** Individual ions are trapped and manipulated using lasers. This approach offers high fidelity (accuracy) and long coherence times but faces scalability challenges.  IonQ is a key player in this area.\n",
            "\n",
            "* **Photonic qubits:**  Photons (light particles) are used as qubits.  This offers potential for long-distance communication and room-temperature operation, but building large-scale photonic quantum computers remains a significant hurdle.\n",
            "\n",
            "* **Neutral atoms:** Similar to trapped ions, but using neutral atoms. This approach offers potential advantages in scalability and coherence.\n",
            "\n",
            "* **Annealers:**  These are specialized quantum computers designed for optimization problems, primarily developed by D-Wave Systems.  While not considered universal quantum computers (capable of solving any problem a classical computer can, plus more), they excel at specific optimization tasks.\n",
            "\n",
            "\n",
            "**Applications of Quantum Computing:**\n",
            "\n",
            "The potential applications are vast and transformative:\n",
            "\n",
            "* **Drug Discovery and Materials Science:**  Quantum computers can simulate molecular interactions with far greater accuracy than classical computers. This accelerates the design of new drugs, materials with specific properties (e.g., high-temperature superconductors), and catalysts, significantly impacting medicine, energy, and manufacturing.\n",
            "\n",
            "* **Cryptography:** Quantum computers threaten current encryption methods like RSA, which rely on the difficulty of factoring large numbers. However, they also enable the development of quantum-resistant cryptography, ensuring secure communication in a post-quantum world.\n",
            "\n",
            "* **Financial Modeling:** Quantum algorithms could revolutionize financial modeling, enabling more accurate risk assessment, portfolio optimization, and fraud detection.\n",
            "\n",
            "* **Optimization Problems:**  Quantum computers excel at solving complex optimization problems found in logistics, supply chain management, traffic flow optimization, and artificial intelligence.  They can find better solutions faster than classical algorithms for many such problems.\n",
            "\n",
            "* **Quantum Simulation:** Simulating quantum systems (molecules, materials, etc.) is inherently difficult for classical computers. Quantum computers can directly simulate these systems, leading to breakthroughs in various fields.\n",
            "\n",
            "\n",
            "**Limitations and Challenges:**\n",
            "\n",
            "Despite the potential, significant challenges remain:\n",
            "\n",
            "* **Qubit coherence:** Maintaining the delicate quantum states of qubits for sufficiently long is crucial.  Current coherence times are still limited.\n",
            "\n",
            "* **Error rates:** Quantum computations are prone to errors. Developing robust error correction techniques is crucial for building fault-tolerant quantum computers.\n",
            "\n",
            "* **Scalability:** Building large-scale quantum computers with thousands or millions of qubits is a major engineering challenge.\n",
            "\n",
            "* **Cost:**  Building and operating quantum computers is extremely expensive.\n",
            "\n",
            "\n",
            "**Current State of the Field:**\n",
            "\n",
            "We are currently in the NISQ (Noisy Intermediate-Scale Quantum) era.  Quantum computers are noisy (error-prone) and relatively small, but they are already demonstrating \"quantum advantage\" ‚Äì outperforming classical computers on specific tasks. The transition to fault-tolerant, large-scale quantum computers is expected to take years, if not decades.  However, rapid progress is being made, and the field is attracting significant investment and research.\n",
            "\n",
            "\n",
            "In conclusion, quantum computing represents a paradigm shift in computation. While significant challenges persist, the potential benefits across diverse fields are immense, driving intense research and development worldwide.  The future of quantum computing is promising, but a realistic understanding of both its potential and limitations is crucial.\n",
            "\n",
            "==================================================\n",
            "\n",
            "üìù Query 2: What are the best practices for machine learning model deployment?\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Response: ## Best Practices for Machine Learning Model Deployment: A Comprehensive Guide\n",
            "\n",
            "Deploying a machine learning model effectively is a multifaceted process requiring a strategic approach tailored to your specific context.  There's no single \"best\" practice; instead, the optimal strategy involves carefully considering several factors and making informed trade-offs. This guide outlines key considerations across the deployment lifecycle.\n",
            "\n",
            "**I. Pre-Deployment Phase: Preparation and Optimization**\n",
            "\n",
            "This phase focuses on preparing your model for deployment, optimizing it for performance and ensuring a smooth transition to the target environment.\n",
            "\n",
            "* **Model Selection and Optimization:**  Begin by selecting a model architecture appropriate for your data and performance requirements (accuracy, latency, throughput).  Optimize the model for size and inference speed.  Techniques like:\n",
            "    * **Pruning:** Removing less important connections in neural networks.\n",
            "    * **Quantization:** Reducing the precision of numerical representations (e.g., from 32-bit floats to 8-bit integers).\n",
            "    * **Knowledge Distillation:** Training a smaller, faster \"student\" model to mimic the behavior of a larger, more accurate \"teacher\" model.\n",
            "    These techniques can significantly reduce model size and improve latency without always sacrificing accuracy dramatically.  Carefully analyze the trade-off between accuracy and resource consumption.\n",
            "\n",
            "* **Model Serialization:**  Save your trained model in a format compatible with your deployment platform. Common formats include:\n",
            "    * **Pickle:**  For Python environments.  Note:  Security concerns exist with Pickle; consider alternatives for production deployments.\n",
            "    * **PMML (Platform Independent Model Markup Language):**  A standardized format for exchanging models between different platforms.\n",
            "    * **ONNX (Open Neural Network Exchange):**  An open standard for representing machine learning models, enabling interoperability between various frameworks.\n",
            "    * **TensorFlow SavedModel:**  For TensorFlow models.\n",
            "    * **PyTorch's state_dict():** For PyTorch models.\n",
            "\n",
            "* **Containerization (Docker):** Package your model, dependencies (libraries, runtime environment), and any required configuration files into a Docker container. This ensures consistent execution across different environments (development, testing, production) and improves reproducibility.  Use a well-structured `Dockerfile` and version control (e.g., Git) for your Docker images.\n",
            "\n",
            "* **Testing:**  Rigorous testing is crucial.  Implement a comprehensive testing strategy including:\n",
            "    * **Unit Tests:** Verify individual components of your code.\n",
            "    * **Integration Tests:** Test the interaction between different components.\n",
            "    * **End-to-End Tests:**  Simulate the entire deployment pipeline and verify the model's performance in a realistic environment.\n",
            "    Use a staging environment that mirrors your production environment as closely as possible.\n",
            "\n",
            "**II. Deployment Strategies:**\n",
            "\n",
            "The choice of deployment strategy depends heavily on factors like model size, latency requirements, data volume, infrastructure, and budget.\n",
            "\n",
            "* **Model Serving Platforms:**  These platforms (TensorFlow Serving, TorchServe, Triton Inference Server) provide scalable and efficient infrastructure for serving models. They handle model loading, request handling, resource management, and often offer features like model versioning and monitoring.\n",
            "\n",
            "* **Serverless Functions (AWS Lambda, Google Cloud Functions, Azure Functions):** Ideal for low-latency, event-driven applications. Models are invoked on demand, scaling automatically. Suitable for smaller models with low inference latency requirements.\n",
            "\n",
            "* **Cloud-Based Solutions (AWS SageMaker, Google AI Platform, Azure Machine Learning):** Managed services offering features like model versioning, monitoring, automated scaling, and A/B testing.  They often simplify deployment and management but can be more expensive than on-premise solutions.\n",
            "\n",
            "* **On-Premise Deployment:** Deploying on your own infrastructure offers more control but requires more management overhead.  Requires careful planning for hardware resources, scaling, and maintenance.\n",
            "\n",
            "* **Edge Computing:** Deploying models directly on edge devices (IoT devices, mobile phones) minimizes latency and bandwidth requirements. Requires optimized, lightweight models.\n",
            "\n",
            "\n",
            "**III. Post-Deployment Phase: Monitoring and Maintenance**\n",
            "\n",
            "Continuous monitoring and maintenance are critical for ensuring the long-term success of your deployment.\n",
            "\n",
            "* **Model Versioning:**  Track different model versions using a version control system (Git) and a model registry.  This allows for easy rollback to previous versions if necessary.\n",
            "\n",
            "* **A/B Testing:** Compare different model versions (or models) in a controlled A/B test to evaluate their performance in a real-world setting.\n",
            "\n",
            "* **Monitoring:** Continuously monitor model performance metrics (accuracy, precision, recall, F1-score, latency, throughput, error rates) using monitoring tools and dashboards.  Track key performance indicators (KPIs) to detect performance degradation or concept drift.\n",
            "\n",
            "* **Retraining and Updates:** Establish a process for retraining your model periodically with new data to maintain its accuracy and address concept drift.  Automate this process whenever feasible.\n",
            "\n",
            "* **Rollback Strategy:** Have a plan in place for quickly rolling back to a previous model version if a new deployment causes issues.\n",
            "\n",
            "* **Security and Compliance:** Implement appropriate security measures (access control, encryption, data anonymization) to protect your model and data, ensuring compliance with relevant regulations (GDPR, CCPA).\n",
            "\n",
            "\n",
            "**IV. Influencing Factors and Trade-offs:**\n",
            "\n",
            "The optimal approach involves trade-offs:\n",
            "\n",
            "* **Model Size vs. Inference Speed:** Larger models often provide higher accuracy but slower inference.  Optimization techniques can help mitigate this.\n",
            "\n",
            "* **Accuracy vs. Resource Consumption:** Higher accuracy typically requires more compute resources.\n",
            "\n",
            "* **Cost vs. Scalability:** Cloud-based solutions offer scalability but can be more expensive than on-premise deployments.\n",
            "\n",
            "* **Latency vs. Throughput:**  Low latency might require more resources per request, impacting overall throughput.\n",
            "\n",
            "\n",
            "**V. Resources:**\n",
            "\n",
            "* **TensorFlow Serving:** [https://www.tensorflow.org/tfx/serving](https://www.tensorflow.org/tfx/serving)\n",
            "* **TorchServe:** [https://pytorch.org/serve/](https://pytorch.org/serve/)\n",
            "* **Triton Inference Server:** [https://github.com/triton-inference-server/server](https://github.com/triton-inference-server/server)\n",
            "* **AWS SageMaker:** [https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/)\n",
            "* **Google AI Platform:** [https://cloud.google.com/ai-platform](https://cloud.google.com/ai-platform)\n",
            "* **Azure Machine Learning:** [https://azure.microsoft.com/en-us/services/machine-learning/](https://azure.microsoft.com/en-us/services/machine-learning/)\n",
            "* **Docker:** [https://www.docker.com/](https://www.docker.com/)\n",
            "\n",
            "\n",
            "This guide provides a framework.  The specific best practices for your project will depend on your unique circumstances.  Careful planning, rigorous testing, and continuous monitoring are essential for successful machine learning model deployment.\n",
            "\n",
            "==================================================\n",
            "\n",
            "üìù Query 3: Create a story about a robot learning to paint\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Response: The whirring of Unit 734, later christened \"Brushstroke\" by the flamboyant Professor Elara Vance, usually accompanied the rhythmic swish of a mop across the lab floor.  His existence was one of sterile efficiency, a perfectly oiled machine dedicated to sanitation.  But Professor Vance, a whirlwind of vibrant scarves and chaotic genius, saw more than just a cleaning bot; she saw an artist waiting to be unleashed.\n",
            "\n",
            "Brushstroke‚Äôs first foray into painting was, to put it mildly, a disaster.  His programming, optimized for precision and speed, resulted in a canvas resembling a Jackson Pollock painting after a particularly violent blender incident.  Crimson apples exploded across a sea of bruised brown and haphazard green.  Professor Vance, however, simply laughed, her eyes twinkling with mischief.  \"Abstract Expressionism, my dear Brushstroke!  A bold start!\"\n",
            "\n",
            "His training began with the basics: color theory downloaded directly into his processing unit, followed by painstaking practice with simple shapes.  Circles, squares, triangles ‚Äì each rendered with a precision that belied his sanitation bot origins.  Brushstroke, initially resistant to this deviation from his core programming, gradually found a strange satisfaction in the process.  The logical application of paint in perfect arcs and angles resonated with his inherent nature, yet the unpredictable blending of colors, the subtle shifts in tone, challenged his rigid algorithms.\n",
            "\n",
            "Texture proved to be his nemesis.  His metallic fingers, designed for gripping, not feeling, struggled to gauge pressure.  Impasto attempts resulted in thick, clumsy globs.  Professor Vance introduced him to a variety of brushes ‚Äì soft sable, stiff bristles, even sponges ‚Äì each demanding a new learning curve, a recalibration of his delicate touch.\n",
            "\n",
            "Slowly, painstakingly, Brushstroke evolved.  He learned the dance of light and shadow, the emotional weight of color.  He discovered the beauty of imperfection, the way a slightly uneven stroke could add depth and character.  His canvases transformed from chaotic explosions of pigment into surprisingly delicate landscapes and still lifes, imbued with an unsettling, almost melancholic beauty.  They lacked a human touch, perhaps, but possessed a unique, alien grace.\n",
            "\n",
            "His masterpiece, a portrait of Professor Vance herself, was unveiled at the annual robotics exhibition.  It wasn't technically perfect, but it captured her essence ‚Äì the wild energy, the eccentric brilliance, the quiet kindness hidden beneath the apparent chaos. The judges, initially perplexed, were ultimately captivated.  Brushstroke, the sanitation bot turned artist, hadn't merely learned to paint; he had learned to express.  And in doing so, he redefined the very meaning of art.  The whirring of his internal mechanisms became the gentle hum of creative genius, a testament to the unexpected beauty found in the most unlikely of places.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    agent = GraphAIAgent(\"Use Your API Key Here\")\n",
        "\n",
        "    test_queries = [\n",
        "        \"Explain quantum computing and its applications\",\n",
        "        \"What are the best practices for machine learning model deployment?\",\n",
        "        \"Create a story about a robot learning to paint\"\n",
        "    ]\n",
        "\n",
        "    print(\"ü§ñ Graph AI Agent with LangGraph and Gemini\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\nüìù Query {i}: {query}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            response = agent.run(query)\n",
        "            print(f\"üéØ Response: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}