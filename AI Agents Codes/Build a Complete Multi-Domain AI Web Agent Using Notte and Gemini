{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install notte python-dotenv pydantic google-generativeai requests beautifulsoup4\n",
        "!patchright install --with-deps chromium\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "GEMINI_API_KEY = \"USE YOUR OWN API KEY HERE\"\n",
        "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "import notte"
      ],
      "metadata": {
        "id": "gkxUAgskStFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductInfo(BaseModel):\n",
        "    name: str\n",
        "    price: str\n",
        "    rating: Optional[float]\n",
        "    availability: str\n",
        "    description: str\n",
        "\n",
        "class NewsArticle(BaseModel):\n",
        "    title: str\n",
        "    summary: str\n",
        "    url: str\n",
        "    date: str\n",
        "    source: str\n",
        "\n",
        "class SocialMediaPost(BaseModel):\n",
        "    content: str\n",
        "    author: str\n",
        "    likes: int\n",
        "    timestamp: str\n",
        "    platform: str\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    query: str\n",
        "    results: List[dict]\n",
        "    total_found: int"
      ],
      "metadata": {
        "id": "PbGPRSwnS0ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedNotteAgent:\n",
        "    def __init__(self, headless=True, max_steps=20):\n",
        "        self.headless = headless\n",
        "        self.max_steps = max_steps\n",
        "        self.session = None\n",
        "        self.agent = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.session = notte.Session(headless=self.headless)\n",
        "        self.session.__enter__()\n",
        "        self.agent = notte.Agent(\n",
        "            session=self.session,\n",
        "            reasoning_model='gemini/gemini-2.5-flash',\n",
        "            max_steps=self.max_steps\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.session:\n",
        "            self.session.__exit__(exc_type, exc_val, exc_tb)\n",
        "\n",
        "    def research_product(self, product_name: str, website: str = \"amazon.com\") -> ProductInfo:\n",
        "        \"\"\"Research a product and extract structured information\"\"\"\n",
        "        task = f\"Go to {website}, search for '{product_name}', click on the first relevant product, and extract detailed product information including name, price, rating, availability, and description.\"\n",
        "\n",
        "        response = self.agent.run(\n",
        "            task=task,\n",
        "            response_format=ProductInfo,\n",
        "            url=f\"https://{website}\"\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "    def news_aggregator(self, topic: str, num_articles: int = 3) -> List[NewsArticle]:\n",
        "        \"\"\"Aggregate news articles on a specific topic\"\"\"\n",
        "        task = f\"Search for recent news about '{topic}', find {num_articles} relevant articles, and extract title, summary, URL, date, and source for each.\"\n",
        "\n",
        "        response = self.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://news.google.com\",\n",
        "            response_format=List[NewsArticle]\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "    def social_media_monitor(self, hashtag: str, platform: str = \"twitter\") -> List[SocialMediaPost]:\n",
        "        \"\"\"Monitor social media for specific hashtags\"\"\"\n",
        "        if platform.lower() == \"twitter\":\n",
        "            url = \"https://twitter.com\"\n",
        "        elif platform.lower() == \"reddit\":\n",
        "            url = \"https://reddit.com\"\n",
        "        else:\n",
        "            url = f\"https://{platform}.com\"\n",
        "\n",
        "        task = f\"Go to {platform}, search for posts with hashtag '{hashtag}', and extract content, author, engagement metrics, and timestamps from the top 5 posts.\"\n",
        "\n",
        "        response = self.agent.run(\n",
        "            task=task,\n",
        "            url=url,\n",
        "            response_format=List[SocialMediaPost]\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "    def competitive_analysis(self, company: str, competitors: List[str]) -> dict:\n",
        "        \"\"\"Perform competitive analysis by gathering pricing and feature data\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for competitor in [company] + competitors:\n",
        "            task = f\"Go to {competitor}'s website, find their pricing page or main product page, and extract key features, pricing tiers, and unique selling points.\"\n",
        "\n",
        "            try:\n",
        "                response = self.agent.run(\n",
        "                    task=task,\n",
        "                    url=f\"https://{competitor}.com\"\n",
        "                )\n",
        "                results[competitor] = response.answer\n",
        "                time.sleep(2)\n",
        "            except Exception as e:\n",
        "                results[competitor] = f\"Error: {str(e)}\"\n",
        "\n",
        "        return results\n",
        "\n",
        "    def job_market_scanner(self, job_title: str, location: str = \"remote\") -> List[dict]:\n",
        "        \"\"\"Scan job market for opportunities\"\"\"\n",
        "        task = f\"Search for '{job_title}' jobs in '{location}', extract job titles, companies, salary ranges, and application URLs from the first 10 results.\"\n",
        "\n",
        "        response = self.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://indeed.com\"\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "    def price_comparison(self, product: str, websites: List[str]) -> dict:\n",
        "        \"\"\"Compare prices across multiple websites\"\"\"\n",
        "        price_data = {}\n",
        "\n",
        "        for site in websites:\n",
        "            task = f\"Search for '{product}' on this website and find the best price, including any discounts or special offers.\"\n",
        "\n",
        "            try:\n",
        "                response = self.agent.run(\n",
        "                    task=task,\n",
        "                    url=f\"https://{site}\"\n",
        "                )\n",
        "                price_data[site] = response.answer\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                price_data[site] = f\"Error: {str(e)}\"\n",
        "\n",
        "        return price_data\n",
        "\n",
        "    def content_research(self, topic: str, content_type: str = \"blog\") -> dict:\n",
        "        \"\"\"Research content ideas and trending topics\"\"\"\n",
        "        if content_type == \"blog\":\n",
        "            url = \"https://medium.com\"\n",
        "            task = f\"Search for '{topic}' articles, analyze trending content, and identify popular themes, engagement patterns, and content gaps.\"\n",
        "        elif content_type == \"video\":\n",
        "            url = \"https://youtube.com\"\n",
        "            task = f\"Search for '{topic}' videos, analyze view counts, titles, and descriptions to identify trending formats and popular angles.\"\n",
        "        else:\n",
        "            url = \"https://google.com\"\n",
        "            task = f\"Search for '{topic}' content across the web and analyze trending discussions and popular formats.\"\n",
        "\n",
        "        response = self.agent.run(task=task, url=url)\n",
        "        return {\"topic\": topic, \"insights\": response.answer, \"platform\": content_type}"
      ],
      "metadata": {
        "id": "WCzlTMClS3Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_ecommerce_research():\n",
        "    \"\"\"Demo: E-commerce product research and comparison\"\"\"\n",
        "    print(\"ğŸ›ï¸ E-commerce Research Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent(headless=True) as agent:\n",
        "        product = agent.research_product(\"wireless earbuds\", \"amazon.com\")\n",
        "        print(f\"Product Research Results:\")\n",
        "        print(f\"Name: {product.name}\")\n",
        "        print(f\"Price: {product.price}\")\n",
        "        print(f\"Rating: {product.rating}\")\n",
        "        print(f\"Availability: {product.availability}\")\n",
        "        print(f\"Description: {product.description[:100]}...\")\n",
        "\n",
        "        print(\"\\nğŸ’° Price Comparison:\")\n",
        "        websites = [\"amazon.com\", \"ebay.com\", \"walmart.com\"]\n",
        "        prices = agent.price_comparison(\"wireless earbuds\", websites)\n",
        "        for site, data in prices.items():\n",
        "            print(f\"{site}: {data}\")\n",
        "\n",
        "def demo_news_intelligence():\n",
        "    \"\"\"Demo: News aggregation and analysis\"\"\"\n",
        "    print(\"ğŸ“° News Intelligence Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent() as agent:\n",
        "        articles = agent.news_aggregator(\"artificial intelligence\", 3)\n",
        "\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            print(f\"\\nArticle {i}:\")\n",
        "            print(f\"Title: {article.title}\")\n",
        "            print(f\"Source: {article.source}\")\n",
        "            print(f\"Summary: {article.summary}\")\n",
        "            print(f\"URL: {article.url}\")\n",
        "\n",
        "def demo_social_listening():\n",
        "    \"\"\"Demo: Social media monitoring and sentiment analysis\"\"\"\n",
        "    print(\"ğŸ‘‚ Social Media Listening Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent() as agent:\n",
        "        posts = agent.social_media_monitor(\"#AI\", \"reddit\")\n",
        "\n",
        "        for i, post in enumerate(posts, 1):\n",
        "            print(f\"\\nPost {i}:\")\n",
        "            print(f\"Author: {post.author}\")\n",
        "            print(f\"Content: {post.content[:100]}...\")\n",
        "            print(f\"Engagement: {post.likes} likes\")\n",
        "            print(f\"Platform: {post.platform}\")\n",
        "\n",
        "def demo_market_intelligence():\n",
        "    \"\"\"Demo: Competitive analysis and market research\"\"\"\n",
        "    print(\"ğŸ“Š Market Intelligence Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent() as agent:\n",
        "        company = \"openai\"\n",
        "        competitors = [\"anthropic\", \"google\"]\n",
        "        analysis = agent.competitive_analysis(company, competitors)\n",
        "\n",
        "        for comp, data in analysis.items():\n",
        "            print(f\"\\n{comp.upper()}:\")\n",
        "            print(f\"Analysis: {str(data)[:200]}...\")\n",
        "\n",
        "def demo_job_market_analysis():\n",
        "    \"\"\"Demo: Job market scanning and analysis\"\"\"\n",
        "    print(\"ğŸ’¼ Job Market Analysis Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent() as agent:\n",
        "        jobs = agent.job_market_scanner(\"python developer\", \"san francisco\")\n",
        "\n",
        "        print(f\"Found {len(jobs)} job opportunities:\")\n",
        "        for job in jobs[:3]:\n",
        "            print(f\"- {job}\")\n",
        "\n",
        "def demo_content_strategy():\n",
        "    \"\"\"Demo: Content research and trend analysis\"\"\"\n",
        "    print(\"âœï¸ Content Strategy Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    with AdvancedNotteAgent() as agent:\n",
        "        blog_research = agent.content_research(\"machine learning\", \"blog\")\n",
        "        video_research = agent.content_research(\"machine learning\", \"video\")\n",
        "\n",
        "        print(\"Blog Content Insights:\")\n",
        "        print(blog_research[\"insights\"][:300] + \"...\")\n",
        "\n",
        "        print(\"\\nVideo Content Insights:\")\n",
        "        print(video_research[\"insights\"][:300] + \"...\")"
      ],
      "metadata": {
        "id": "8J3I02_yS9jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkflowManager:\n",
        "    def __init__(self):\n",
        "        self.agents = []\n",
        "        self.results = {}\n",
        "\n",
        "    def add_agent_task(self, name: str, task_func, *args, **kwargs):\n",
        "        \"\"\"Add an agent task to the workflow\"\"\"\n",
        "        self.agents.append({\n",
        "            'name': name,\n",
        "            'func': task_func,\n",
        "            'args': args,\n",
        "            'kwargs': kwargs\n",
        "        })\n",
        "\n",
        "    def execute_workflow(self, parallel=False):\n",
        "        \"\"\"Execute all agent tasks in the workflow\"\"\"\n",
        "        print(\"ğŸš€ Executing Multi-Agent Workflow\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for agent_task in self.agents:\n",
        "            name = agent_task['name']\n",
        "            func = agent_task['func']\n",
        "            args = agent_task['args']\n",
        "            kwargs = agent_task['kwargs']\n",
        "\n",
        "            print(f\"\\nğŸ¤– Executing {name}...\")\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "                self.results[name] = result\n",
        "                print(f\"âœ… {name} completed successfully\")\n",
        "            except Exception as e:\n",
        "                self.results[name] = f\"Error: {str(e)}\"\n",
        "                print(f\"âŒ {name} failed: {str(e)}\")\n",
        "\n",
        "            if not parallel:\n",
        "                time.sleep(2)\n",
        "        return self.results\n",
        "\n",
        "def market_research_workflow(company_name: str, product_category: str):\n",
        "    \"\"\"Complete market research workflow\"\"\"\n",
        "    workflow = WorkflowManager()\n",
        "\n",
        "    workflow.add_agent_task(\n",
        "        \"Product Research\",\n",
        "        lambda: research_trending_products(product_category)\n",
        "    )\n",
        "\n",
        "    workflow.add_agent_task(\n",
        "        \"Competitive Analysis\",\n",
        "        lambda: analyze_competitors(company_name, product_category)\n",
        "    )\n",
        "\n",
        "    workflow.add_agent_task(\n",
        "        \"Social Sentiment\",\n",
        "        lambda: monitor_brand_sentiment(company_name)\n",
        "    )\n",
        "\n",
        "    return workflow.execute_workflow()\n",
        "\n",
        "def research_trending_products(category: str):\n",
        "    \"\"\"Research trending products in a category\"\"\"\n",
        "    with AdvancedNotteAgent(headless=True) as agent:\n",
        "        task = f\"Research trending {category} products, find top 5 products with prices, ratings, and key features.\"\n",
        "        response = agent.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://amazon.com\"\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "def analyze_competitors(company: str, category: str):\n",
        "    \"\"\"Analyze competitors in the market\"\"\"\n",
        "    with AdvancedNotteAgent(headless=True) as agent:\n",
        "        task = f\"Research {company} competitors in {category}, compare pricing strategies, features, and market positioning.\"\n",
        "        response = agent.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://google.com\"\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "def monitor_brand_sentiment(brand: str):\n",
        "    \"\"\"Monitor brand sentiment across platforms\"\"\"\n",
        "    with AdvancedNotteAgent(headless=True) as agent:\n",
        "        task = f\"Search for recent mentions of {brand} on social media and news, analyze sentiment and key themes.\"\n",
        "        response = agent.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://reddit.com\"\n",
        "        )\n",
        "        return response.answer"
      ],
      "metadata": {
        "id": "-aHWUo9UTNFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run all demos\"\"\"\n",
        "    print(\"ğŸš€ Advanced Notte AI Agent Tutorial\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Note: Make sure to set your GEMINI_API_KEY above!\")\n",
        "    print(\"Get your free API key at: https://makersuite.google.com/app/apikey\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY\":\n",
        "        print(\"âŒ Please set your GEMINI_API_KEY in the code above!\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(\"\\n1. E-commerce Research Demo\")\n",
        "        demo_ecommerce_research()\n",
        "\n",
        "        print(\"\\n2. News Intelligence Demo\")\n",
        "        demo_news_intelligence()\n",
        "\n",
        "        print(\"\\n3. Social Media Listening Demo\")\n",
        "        demo_social_listening()\n",
        "\n",
        "        print(\"\\n4. Market Intelligence Demo\")\n",
        "        demo_market_intelligence()\n",
        "\n",
        "        print(\"\\n5. Job Market Analysis Demo\")\n",
        "        demo_job_market_analysis()\n",
        "\n",
        "        print(\"\\n6. Content Strategy Demo\")\n",
        "        demo_content_strategy()\n",
        "\n",
        "        print(\"\\n7. Multi-Agent Workflow Demo\")\n",
        "        results = market_research_workflow(\"Tesla\", \"electric vehicles\")\n",
        "        print(\"Workflow Results:\")\n",
        "        for task, result in results.items():\n",
        "            print(f\"{task}: {str(result)[:150]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during execution: {str(e)}\")\n",
        "        print(\"ğŸ’¡ Tip: Make sure your Gemini API key is valid and you have internet connection\")\n",
        "\n",
        "def quick_scrape(url: str, instructions: str = \"Extract main content\"):\n",
        "    \"\"\"Quick scraping function for simple data extraction\"\"\"\n",
        "    with AdvancedNotteAgent(headless=True, max_steps=5) as agent:\n",
        "        response = agent.agent.run(\n",
        "            task=f\"{instructions} from this webpage\",\n",
        "            url=url\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "def quick_search(query: str, num_results: int = 5):\n",
        "    \"\"\"Quick search function with structured results\"\"\"\n",
        "    with AdvancedNotteAgent(headless=True, max_steps=10) as agent:\n",
        "        task = f\"Search for '{query}' and return the top {num_results} results with titles, URLs, and brief descriptions.\"\n",
        "        response = agent.agent.run(\n",
        "            task=task,\n",
        "            url=\"https://google.com\",\n",
        "            response_format=SearchResult\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "def quick_form_fill(form_url: str, form_data: dict):\n",
        "    \"\"\"Quick form filling function\"\"\"\n",
        "    with AdvancedNotteAgent(headless=False, max_steps=15) as agent:\n",
        "        data_str = \", \".join([f\"{k}: {v}\" for k, v in form_data.items()])\n",
        "        task = f\"Fill out the form with this information: {data_str}, then submit it.\"\n",
        "\n",
        "        response = agent.agent.run(\n",
        "            task=task,\n",
        "            url=form_url\n",
        "        )\n",
        "        return response.answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ§ª Quick Test Examples:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    print(\"1. Quick Scrape Example:\")\n",
        "    try:\n",
        "        result = quick_scrape(\"https://news.ycombinator.com\", \"Extract the top 3 post titles\")\n",
        "        print(f\"Scraped: {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\n2. Quick Search Example:\")\n",
        "    try:\n",
        "        search_results = quick_search(\"latest AI news\", 3)\n",
        "        print(f\"Search Results: {search_results}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\n3. Custom Agent Task:\")\n",
        "    try:\n",
        "        with AdvancedNotteAgent(headless=True) as agent:\n",
        "            response = agent.agent.run(\n",
        "                task=\"Go to Wikipedia, search for 'artificial intelligence', and summarize the main article in 2 sentences.\",\n",
        "                url=\"https://wikipedia.org\"\n",
        "            )\n",
        "            print(f\"Wikipedia Summary: {response.answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    main()\n",
        "\n",
        "    print(\"\\nâœ¨ Tutorial Complete!\")\n",
        "    print(\"ğŸ’¡ Tips for success:\")\n",
        "    print(\"- Start with simple tasks and gradually increase complexity\")\n",
        "    print(\"- Use structured outputs (Pydantic models) for reliable data extraction\")\n",
        "    print(\"- Implement rate limiting to respect API quotas\")\n",
        "    print(\"- Handle errors gracefully in production workflows\")\n",
        "    print(\"- Combine scripting with AI for cost-effective automation\")\n",
        "\n",
        "    print(\"\\nğŸš€ Next Steps:\")\n",
        "    print(\"- Customize the agents for your specific use cases\")\n",
        "    print(\"- Add error handling and retry logic for production\")\n",
        "    print(\"- Implement logging and monitoring for agent activities\")\n",
        "    print(\"- Scale up with Notte's hosted API service for enterprise features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XVlEWWxMQkO8",
        "outputId": "9b92aaf5-c671-42dd-f11f-a0619e02249e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting notte\n",
            "  Downloading notte-1.6.8-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting google-auth>=2.39.0 (from notte)\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting notte-agent==v1.6.8 (from notte)\n",
            "  Downloading notte_agent-1.6.8-py3-none-any.whl.metadata (222 bytes)\n",
            "Collecting notte-browser==v1.6.8 (from notte)\n",
            "  Downloading notte_browser-1.6.8-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: notte-core==v1.6.8 in /usr/local/lib/python3.12/dist-packages (from notte) (1.6.8)\n",
            "Requirement already satisfied: notte-sdk==v1.6.8 in /usr/local/lib/python3.12/dist-packages (from notte) (1.6.8)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from notte) (0.10.2)\n",
            "Collecting maincontentextractor (from notte-browser==v1.6.8->notte)\n",
            "  Downloading MainContentExtractor-0.0.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdownify>=0.14.1 (from notte-browser==v1.6.8->notte)\n",
            "  Downloading markdownify-1.2.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting patchright==1.50.0 (from notte-browser==v1.6.8->notte)\n",
            "  Downloading patchright-1.50.0-py3-none-manylinux1_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: chevron>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=43.0.3 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (43.0.3)\n",
            "Requirement already satisfied: litellm>=1.74.12 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (1.76.1)\n",
            "Requirement already satisfied: llamux>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (0.1.9)\n",
            "Requirement already satisfied: loguru>=0.7.3 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (0.7.3)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (1.6.0)\n",
            "Requirement already satisfied: openai<1.100.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (1.99.9)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (1.36.0)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (11.3.0)\n",
            "Requirement already satisfied: posthog>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (6.7.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (2.10.1)\n",
            "Requirement already satisfied: pyotp>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (2.9.0)\n",
            "Requirement already satisfied: restrictedpython>=8.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (8.0)\n",
            "Requirement already satisfied: scarf-sdk>=0.1.2 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (0.1.2)\n",
            "Requirement already satisfied: tldextract>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from notte-core==v1.6.8->notte) (5.3.0)\n",
            "Requirement already satisfied: halo>=0.0.28 in /usr/local/lib/python3.12/dist-packages (from notte-sdk==v1.6.8->notte) (0.0.31)\n",
            "Requirement already satisfied: websockets>=13.1 in /usr/local/lib/python3.12/dist-packages (from notte-sdk==v1.6.8->notte) (15.0.1)\n",
            "Collecting pyee<13,>=12 (from patchright==1.50.0->notte-browser==v1.6.8->notte)\n",
            "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from patchright==1.50.0->notte-browser==v1.6.8->notte) (3.2.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.179.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.39.0->notte) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.39.0->notte) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.39.0->notte) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=43.0.3->notte-core==v1.6.8->notte) (1.17.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: log-symbols>=0.0.14 in /usr/local/lib/python3.12/dist-packages (from halo>=0.0.28->notte-sdk==v1.6.8->notte) (0.0.14)\n",
            "Requirement already satisfied: spinners>=0.0.24 in /usr/local/lib/python3.12/dist-packages (from halo>=0.0.28->notte-sdk==v1.6.8->notte) (0.0.24)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from halo>=0.0.28->notte-sdk==v1.6.8->notte) (3.1.0)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from halo>=0.0.28->notte-sdk==v1.6.8->notte) (0.4.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from halo>=0.0.28->notte-sdk==v1.6.8->notte) (1.17.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (3.12.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (8.2.1)\n",
            "Requirement already satisfied: fastuuid>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (0.12.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (4.25.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (0.11.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.74.12->notte-core==v1.6.8->notte) (0.21.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<1.100.0->notte-core==v1.6.8->notte) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<1.100.0->notte-core==v1.6.8->notte) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<1.100.0->notte-core==v1.6.8->notte) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<1.100.0->notte-core==v1.6.8->notte) (1.3.1)\n",
            "Requirement already satisfied: opentelemetry-api==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.34.1->notte-core==v1.6.8->notte) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.34.1->notte-core==v1.6.8->notte) (0.57b0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from posthog>=3.0.1->notte-core==v1.6.8->notte) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=3.0.1->notte-core==v1.6.8->notte) (2.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.39.0->notte) (0.6.1)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract>=5.3.0->notte-core==v1.6.8->notte) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=5.3.0->notte-core==v1.6.8->notte) (3.19.1)\n",
            "Collecting trafilatura>=1.6.2 (from maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting html2text>=2020.1.16 (from maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.74.12->notte-core==v1.6.8->notte) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=43.0.3->notte-core==v1.6.8->notte) (2.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.74.12->notte-core==v1.6.8->notte) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (0.27.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm>=1.74.12->notte-core==v1.6.8->notte) (2024.11.6)\n",
            "Collecting courlan>=1.3.2 (from trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte) (5.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.74.12->notte-core==v1.6.8->notte) (0.34.4)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.74.12->notte-core==v1.6.8->notte) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.74.12->notte-core==v1.6.8->notte) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.74.12->notte-core==v1.6.8->notte) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.74.12->notte-core==v1.6.8->notte) (1.1.8)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte) (2025.2)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte) (5.3.1)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura>=1.6.2->maincontentextractor->notte-browser==v1.6.8->notte)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading notte-1.6.8-py3-none-any.whl (17 kB)\n",
            "Downloading notte_agent-1.6.8-py3-none-any.whl (33 kB)\n",
            "Downloading notte_browser-1.6.8-py3-none-any.whl (90 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 90.0/90.0 kB 5.4 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading patchright-1.50.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 45.1/45.1 MB 7.3 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 216.1/216.1 kB 9.8 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading markdownify-1.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading MainContentExtractor-0.0.4-py3-none-any.whl (5.7 kB)\n",
            "Downloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
            "Downloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 132.6/132.6 kB 9.2 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 837.9/837.9 kB 39.7 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 315.5/315.5 kB 17.0 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 274.7/274.7 kB 16.7 MB/s eta 0:00:00\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tld, pyee, lxml_html_clean, html2text, patchright, markdownify, google-auth, dateparser, courlan, justext, htmldate, trafilatura, maincontentextractor, notte-browser, notte-agent, notte\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.38.0\n",
            "    Uninstalling google-auth-2.38.0:\n",
            "      Successfully uninstalled google-auth-2.38.0\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
            "Successfully installed courlan-1.3.2 dateparser-1.2.2 google-auth-2.40.3 html2text-2025.4.15 htmldate-1.9.3 justext-3.0.2 lxml_html_clean-0.4.2 maincontentextractor-0.0.4 markdownify-1.2.0 notte-1.6.8 notte-agent-1.6.8 notte-browser-1.6.8 patchright-1.50.0 pyee-12.1.1 tld-0.13.1 trafilatura-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cc8109221c164dbd951191da4715dc6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,942 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,297 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,209 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,340 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,617 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,543 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,783 kB]\n",
            "Fetched 35.0 MB in 4s (8,266 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.3).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.6).\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.15).\n",
            "The following additional packages will be installed:\n",
            "  xfonts-encodings xfonts-utils\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf fonts-ipafont-gothic fonts-noto-color-emoji\n",
            "  fonts-tlwg-loma-otf fonts-unifont fonts-wqy-zenhei xfonts-cyrillic\n",
            "  xfonts-encodings xfonts-scalable xfonts-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 28.4 MB of archives.\n",
            "After this operation, 67.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.047-0ubuntu0.22.04.1 [10.0 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Fetched 28.4 MB in 1s (19.8 MB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../1-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../2-fonts-noto-color-emoji_2.047-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../3-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../4-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../5-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../6-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../7-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../8-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../9-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Downloading Chromium 133.0.6943.16 (playwright build v1155) from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-linux.zip\n",
            "163.5 MiB [] 0% 0.0s163.5 MiB [] 0% 9.8s163.5 MiB [] 0% 6.4s163.5 MiB [] 1% 4.8s163.5 MiB [] 1% 4.5s163.5 MiB [] 2% 4.0s163.5 MiB [] 2% 3.8s163.5 MiB [] 3% 3.6s163.5 MiB [] 3% 3.5s163.5 MiB [] 4% 3.4s163.5 MiB [] 4% 3.5s163.5 MiB [] 5% 3.5s163.5 MiB [] 5% 3.4s163.5 MiB [] 6% 3.3s163.5 MiB [] 6% 3.5s163.5 MiB [] 7% 3.5s163.5 MiB [] 7% 3.8s163.5 MiB [] 7% 3.9s163.5 MiB [] 7% 3.8s163.5 MiB [] 8% 3.6s163.5 MiB [] 9% 3.5s163.5 MiB [] 9% 3.4s163.5 MiB [] 9% 3.5s163.5 MiB [] 10% 3.6s163.5 MiB [] 10% 3.7s163.5 MiB [] 11% 3.6s163.5 MiB [] 12% 3.5s163.5 MiB [] 13% 3.4s163.5 MiB [] 14% 3.4s163.5 MiB [] 15% 3.2s163.5 MiB [] 16% 3.1s163.5 MiB [] 17% 3.0s163.5 MiB [] 18% 3.0s163.5 MiB [] 18% 2.9s163.5 MiB [] 19% 2.8s163.5 MiB [] 20% 2.7s163.5 MiB [] 21% 2.7s163.5 MiB [] 21% 2.6s163.5 MiB [] 22% 2.6s163.5 MiB [] 23% 2.5s163.5 MiB [] 24% 2.4s163.5 MiB [] 25% 2.4s163.5 MiB [] 26% 2.3s163.5 MiB [] 27% 2.3s163.5 MiB [] 28% 2.3s163.5 MiB [] 29% 2.2s163.5 MiB [] 30% 2.2s163.5 MiB [] 31% 2.2s163.5 MiB [] 32% 2.1s163.5 MiB [] 32% 2.2s163.5 MiB [] 33% 2.2s163.5 MiB [] 34% 2.2s163.5 MiB [] 35% 2.1s163.5 MiB [] 37% 2.0s163.5 MiB [] 38% 1.9s163.5 MiB [] 39% 1.9s163.5 MiB [] 40% 1.8s163.5 MiB [] 41% 1.8s163.5 MiB [] 42% 1.7s163.5 MiB [] 43% 1.7s163.5 MiB [] 44% 1.7s163.5 MiB [] 44% 1.6s163.5 MiB [] 46% 1.6s163.5 MiB [] 47% 1.5s163.5 MiB [] 48% 1.5s163.5 MiB [] 49% 1.5s163.5 MiB [] 50% 1.4s163.5 MiB [] 51% 1.4s163.5 MiB [] 52% 1.4s163.5 MiB [] 53% 1.3s163.5 MiB [] 54% 1.3s163.5 MiB [] 55% 1.2s163.5 MiB [] 56% 1.2s163.5 MiB [] 57% 1.2s163.5 MiB [] 58% 1.1s163.5 MiB [] 59% 1.1s163.5 MiB [] 60% 1.1s163.5 MiB [] 61% 1.1s163.5 MiB [] 62% 1.1s163.5 MiB [] 62% 1.0s163.5 MiB [] 63% 1.0s163.5 MiB [] 64% 1.0s163.5 MiB [] 65% 1.0s163.5 MiB [] 66% 1.0s163.5 MiB [] 67% 0.9s163.5 MiB [] 68% 0.9s163.5 MiB [] 69% 0.9s163.5 MiB [] 70% 0.8s163.5 MiB [] 71% 0.8s163.5 MiB [] 72% 0.8s163.5 MiB [] 73% 0.7s163.5 MiB [] 74% 0.7s163.5 MiB [] 75% 0.7s163.5 MiB [] 76% 0.7s163.5 MiB [] 77% 0.6s163.5 MiB [] 78% 0.6s163.5 MiB [] 79% 0.6s163.5 MiB [] 80% 0.6s163.5 MiB [] 80% 0.5s163.5 MiB [] 81% 0.5s163.5 MiB [] 82% 0.5s163.5 MiB [] 83% 0.4s163.5 MiB [] 84% 0.4s163.5 MiB [] 85% 0.4s163.5 MiB [] 86% 0.4s163.5 MiB [] 87% 0.3s163.5 MiB [] 88% 0.3s163.5 MiB [] 89% 0.3s163.5 MiB [] 90% 0.3s163.5 MiB [] 90% 0.2s163.5 MiB [] 91% 0.2s163.5 MiB [] 92% 0.2s163.5 MiB [] 93% 0.2s163.5 MiB [] 94% 0.1s163.5 MiB [] 95% 0.1s163.5 MiB [] 96% 0.1s163.5 MiB [] 97% 0.1s163.5 MiB [] 98% 0.0s163.5 MiB [] 99% 0.0s163.5 MiB [] 100% 0.0s\n",
            "Chromium 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium-1155\n",
            "Downloading FFMPEG playwright build v1011 from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\n",
            "2.3 MiB [] 0% 0.0s2.3 MiB [] 13% 0.1s2.3 MiB [] 46% 0.0s2.3 MiB [] 100% 0.0s\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 133.0.6943.16 (playwright build v1155) from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-headless-shell-linux.zip\n",
            "99.9 MiB [] 0% 0.0s99.9 MiB [] 0% 6.0s99.9 MiB [] 0% 3.6s99.9 MiB [] 1% 2.7s99.9 MiB [] 2% 2.5s99.9 MiB [] 3% 2.5s99.9 MiB [] 3% 2.4s99.9 MiB [] 4% 2.4s99.9 MiB [] 5% 2.2s99.9 MiB [] 6% 2.0s99.9 MiB [] 7% 1.9s99.9 MiB [] 8% 1.9s99.9 MiB [] 9% 1.9s99.9 MiB [] 9% 2.0s99.9 MiB [] 10% 2.0s99.9 MiB [] 10% 2.2s99.9 MiB [] 11% 2.1s99.9 MiB [] 11% 2.2s99.9 MiB [] 12% 2.3s99.9 MiB [] 13% 2.1s99.9 MiB [] 15% 2.0s99.9 MiB [] 17% 1.9s99.9 MiB [] 18% 1.9s99.9 MiB [] 19% 1.9s99.9 MiB [] 20% 1.8s99.9 MiB [] 22% 1.7s99.9 MiB [] 23% 1.7s99.9 MiB [] 24% 1.7s99.9 MiB [] 25% 1.7s99.9 MiB [] 26% 1.6s99.9 MiB [] 28% 1.5s99.9 MiB [] 29% 1.4s99.9 MiB [] 31% 1.4s99.9 MiB [] 32% 1.3s99.9 MiB [] 33% 1.3s99.9 MiB [] 35% 1.3s99.9 MiB [] 36% 1.2s99.9 MiB [] 37% 1.2s99.9 MiB [] 39% 1.1s99.9 MiB [] 40% 1.1s99.9 MiB [] 41% 1.1s99.9 MiB [] 43% 1.0s99.9 MiB [] 44% 1.0s99.9 MiB [] 45% 1.0s99.9 MiB [] 46% 1.0s99.9 MiB [] 47% 1.0s99.9 MiB [] 48% 0.9s99.9 MiB [] 50% 0.9s99.9 MiB [] 52% 0.9s99.9 MiB [] 54% 0.8s99.9 MiB [] 55% 0.8s99.9 MiB [] 57% 0.7s99.9 MiB [] 59% 0.7s99.9 MiB [] 60% 0.7s99.9 MiB [] 62% 0.6s99.9 MiB [] 64% 0.6s99.9 MiB [] 66% 0.5s99.9 MiB [] 67% 0.5s99.9 MiB [] 69% 0.5s99.9 MiB [] 70% 0.5s99.9 MiB [] 72% 0.4s99.9 MiB [] 74% 0.4s99.9 MiB [] 76% 0.4s99.9 MiB [] 77% 0.3s99.9 MiB [] 79% 0.3s99.9 MiB [] 81% 0.3s99.9 MiB [] 83% 0.2s99.9 MiB [] 85% 0.2s99.9 MiB [] 87% 0.2s99.9 MiB [] 89% 0.2s99.9 MiB [] 90% 0.1s99.9 MiB [] 92% 0.1s99.9 MiB [] 93% 0.1s99.9 MiB [] 94% 0.1s99.9 MiB [] 96% 0.1s99.9 MiB [] 97% 0.0s99.9 MiB [] 98% 0.0s99.9 MiB [] 99% 0.0s99.9 MiB [] 100% 0.0s\n",
            "Chromium Headless Shell 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1155\n",
            "ğŸ§ª Quick Test Examples:\n",
            "==============================\n",
            "1. Quick Scrape Example:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m\u001b[1mWARNING \u001b[0m - \u001b[33m\u001b[1mğŸªŸ Headless mode detected. Setting default viewport width and height to 1280x1080 to avoid issues.\u001b[0m\n",
            "\u001b[33m\u001b[1mWARNING \u001b[0m - \u001b[33m\u001b[1mğŸªŸ [Browser Settings] Launching browser in headless without providing a user-agent, for better odds at evading bot detection, set a user-agent or run in headful mode\u001b[0m\n",
            "INFO     - Session url is 'about:blank': returning empty observation. Perform goto action before observing to get a more meaningful observation.\n",
            "INFO     - Goto for url='https://news.ycombinator.com' succeeded with HTTP 200: \n",
            "INFO     - ğŸ’¡ Step 1\n",
            "INFO     - ğŸ“ Current page: The page displays a list of news articles with their titles, associated websites, points, authors, time posted, and number of comments. There are also navigation links at the top.\n",
            "INFO     - ğŸ”¬ Previous goal: âœ… Successfully navigated to the Hacker News homepage.\n",
            "INFO     - ğŸ§  Memory: Successfully navigated to Hacker News. The next step is to extract the top 3 post titles.\n",
            "INFO     - ğŸ¯ Next goal: Extract the top 3 post titles from the webpage.\n",
            "INFO     - ğŸ†” Relevant ids:\n",
            "   â–¶ L12: This is the first post title on the page.\n",
            "   â–¶ L19: This is the second post title on the page.\n",
            "   â–¶ L26: This is the third post title on the page.\n",
            "INFO     - âš¡ Taking action:\n",
            "   â–¶ completion with answer=The top 3 post titles are: 1. Jujutsu for Everyone, 2. \"This telegram must be closely paraphrased before being communicated\" Why?, 3. The Sun Sets on the British Empire\n",
            "INFO     - ğŸ”¥ Validating agent output:\n",
            "The top 3 post titles are: 1. Jujutsu for Everyone, 2. \"This telegram must be closely paraphrased before being communicated\" Why?, 3. The Sun Sets on the British Empire\n",
            "INFO     - Validation successful: The agent successfully extracted the top 3 post titles as requested: \"Jujutsu for Everyone\", \"\"This telegram must be closely paraphrased before being communicated\" Why?\", and \"The Sun Sets on the British Empire\".\n",
            "INFO     - âœ… Task completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped: The top 3 post titles are: 1. Jujutsu for Everyone, 2. \"This telegram must be closely paraphrased before being communicated\" Why?, 3. The Sun Sets on the British Empire\n",
            "\n",
            "2. Quick Search Example:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING  - ğŸªŸ Headless mode detected. Setting default viewport width and height to 1280x1080 to avoid issues.\n",
            "WARNING  - ğŸªŸ [Browser Settings] Launching browser in headless without providing a user-agent, for better odds at evading bot detection, set a user-agent or run in headful mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 1 validation error for AgentRunRequest\n",
            "response_format\n",
            "  Value error, The provided response format cannot be handled by notte at the moment [type=value_error, input_value={'properties': {'query': ...sult', 'type': 'object'}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
            "\n",
            "3. Custom Agent Task:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING  - ğŸªŸ Headless mode detected. Setting default viewport width and height to 1280x1080 to avoid issues.\n",
            "WARNING  - ğŸªŸ [Browser Settings] Launching browser in headless without providing a user-agent, for better odds at evading bot detection, set a user-agent or run in headful mode\n",
            "INFO     - Session url is 'about:blank': returning empty observation. Perform goto action before observing to get a more meaningful observation.\n",
            "INFO     - Goto for url='https://wikipedia.org' succeeded with HTTP 301: \n",
            "INFO     - ğŸ’¡ Step 1\n",
            "INFO     - ğŸ“ Current page: The page is the Wikipedia homepage, displaying various language options and a search bar. It also shows links to other Wikimedia projects.\n",
            "INFO     - ğŸ”¬ Previous goal: âœ… Successfully navigated to the Wikipedia homepage.\n",
            "INFO     - ğŸ§  Memory: Navigated to Wikipedia homepage. Next, I need to search for 'artificial intelligence'.\n",
            "INFO     - ğŸ¯ Next goal: Type 'artificial intelligence' into the search bar and click the search button.\n",
            "INFO     - ğŸ†” Relevant ids:\n",
            "   â–¶ I1: This is the search input field where I need to type 'artificial intelligence'.\n",
            "   â–¶ B1: This is the search button to submit the query after typing in the search field.\n",
            "INFO     - âš¡ Taking action:\n",
            "   â–¶ fill with id I1\n",
            "INFO     - âœ… action 'fill' with id=I1 succeeded: 'Filled the input field 'search' with the value: 'artificial intelligence''\n",
            "\n",
            "\n",
            "INFO     - ğŸ’¡ Step 2\n",
            "INFO     - ğŸ“ Current page: The Wikipedia homepage is displayed with a search bar that has been pre-filled with 'artificial intelligence'. Several search suggestions related to 'Artificial intelligence' are shown below the search bar, including a direct link to the 'Artificial intelligence' article.\n",
            "INFO     - ğŸ”¬ Previous goal: âœ… I successfully filled the search bar with 'artificial intelligence'. The page now shows search suggestions.\n",
            "INFO     - ğŸ§  Memory: Navigated to Wikipedia homepage and searched for 'artificial intelligence'. Next, I need to go to the main article and summarize it.\n",
            "INFO     - ğŸ¯ Next goal: Click on the link that leads to the main 'Artificial intelligence' article.\n",
            "INFO     - ğŸ†” Relevant ids:\n",
            "   â–¶ L11: This link directly leads to the 'Artificial intelligence' article, which is the main goal.\n",
            "   â–¶ B1: This is the search button, which would submit the search query.\n",
            "INFO     - âš¡ Taking action:\n",
            "   â–¶ click with id L11\n",
            "INFO     - âœ… action 'click' with id=L11 succeeded: 'Clicked on the element with text label: Artificial intelligence Intelligence of machines'\n",
            "\n",
            "\n",
            "INFO     - ğŸ’¡ Step 3\n",
            "INFO     - ğŸ“ Current page: The page is the Wikipedia article on Artificial intelligence. It defines AI as the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, and problem-solving. It also mentions that AI is a field of research in computer science that develops methods and software enabling machines to perceive their environment and take actions to achieve defined goals. The article discusses various applications and historical context of AI.\n",
            "INFO     - ğŸ”¬ Previous goal: âœ… Successfully navigated to the 'Artificial intelligence' Wikipedia page.\n",
            "INFO     - ğŸ§  Memory: Navigated to Wikipedia homepage, searched for 'artificial intelligence', and clicked on the main article. Now I need to summarize the main article in 2 sentences.\n",
            "INFO     - ğŸ¯ Next goal: Summarize the main article on 'Artificial intelligence' in two sentences and complete the task.\n",
            "INFO     - ğŸ†” Relevant ids:\n",
            "INFO     - âš¡ Taking action:\n",
            "   â–¶ completion with answer=Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, and problem-solving. It is a field of research in computer science that develops methods and software enabling machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\n",
            "INFO     - ğŸ”¥ Validating agent output:\n",
            "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, and problem-solving. It is a field of research in computer science that develops methods and software enabling machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\n",
            "INFO     - Validation successful: The agent successfully navigated to Wikipedia, searched for 'artificial intelligence', and provided a two-sentence summary directly from the beginning of the article, which accurately reflects the main points.\n",
            "INFO     - âœ… Task completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia Summary: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, and problem-solving. It is a field of research in computer science that develops methods and software enabling machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\n",
            "\n",
            "âœ¨ Tutorial Complete!\n",
            "ğŸ’¡ Tips for success:\n",
            "- Start with simple tasks and gradually increase complexity\n",
            "- Use structured outputs (Pydantic models) for reliable data extraction\n",
            "- Implement rate limiting to respect API quotas\n",
            "- Handle errors gracefully in production workflows\n",
            "- Combine scripting with AI for cost-effective automation\n",
            "\n",
            "ğŸš€ Next Steps:\n",
            "- Customize the agents for your specific use cases\n",
            "- Add error handling and retry logic for production\n",
            "- Implement logging and monitoring for agent activities\n",
            "- Scale up with Notte's hosted API service for enterprise features\n"
          ]
        }
      ]
    }
  ]
}