{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required packages for DeepSpeed in Colab\"\"\"\n",
        "    print(\"ğŸš€ Installing DeepSpeed and dependencies...\")\n",
        "\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\",\n",
        "        \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\",\n",
        "        \"https://download.pytorch.org/whl/cu118\"\n",
        "    ])\n",
        "\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"deepspeed\"])\n",
        "\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\",\n",
        "        \"transformers\", \"datasets\", \"accelerate\", \"wandb\"\n",
        "    ])\n",
        "\n",
        "    print(\"âœ… Installation complete!\")\n",
        "\n",
        "install_dependencies()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import deepspeed\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrg0oJmZ9-Wv",
        "outputId": "f6cad8c4-4861-4151-a8e0-e00427bdf95b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Installing DeepSpeed and dependencies...\n",
            "âœ… Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticTextDataset(Dataset):\n",
        "    \"\"\"Synthetic dataset for demonstration purposes\"\"\"\n",
        "\n",
        "    def __init__(self, size: int = 1000, seq_length: int = 512, vocab_size: int = 50257):\n",
        "        self.size = size\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.data = torch.randint(0, vocab_size, (size, seq_length))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.data[idx],\n",
        "            'labels': self.data[idx].clone()\n",
        "        }"
      ],
      "metadata": {
        "id": "IUM9PaXj-HXU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDeepSpeedTrainer:\n",
        "    \"\"\"Advanced DeepSpeed trainer with multiple optimization techniques\"\"\"\n",
        "\n",
        "    def __init__(self, model_config: Dict[str, Any], ds_config: Dict[str, Any]):\n",
        "        self.model_config = model_config\n",
        "        self.ds_config = ds_config\n",
        "        self.model = None\n",
        "        self.engine = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"Create a GPT-2 style model for demonstration\"\"\"\n",
        "        print(\"ğŸ§  Creating model...\")\n",
        "\n",
        "        config = GPT2Config(\n",
        "            vocab_size=self.model_config['vocab_size'],\n",
        "            n_positions=self.model_config['seq_length'],\n",
        "            n_embd=self.model_config['hidden_size'],\n",
        "            n_layer=self.model_config['num_layers'],\n",
        "            n_head=self.model_config['num_heads'],\n",
        "            resid_pdrop=0.1,\n",
        "            embd_pdrop=0.1,\n",
        "            attn_pdrop=0.1,\n",
        "        )\n",
        "\n",
        "        self.model = GPT2LMHeadModel(config)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        return self.model\n",
        "\n",
        "    def create_deepspeed_config(self):\n",
        "        \"\"\"Create comprehensive DeepSpeed configuration\"\"\"\n",
        "        return {\n",
        "            \"train_batch_size\": self.ds_config['train_batch_size'],\n",
        "            \"train_micro_batch_size_per_gpu\": self.ds_config['micro_batch_size'],\n",
        "            \"gradient_accumulation_steps\": self.ds_config['gradient_accumulation_steps'],\n",
        "\n",
        "            \"zero_optimization\": {\n",
        "                \"stage\": self.ds_config['zero_stage'],\n",
        "                \"allgather_partitions\": True,\n",
        "                \"allgather_bucket_size\": 5e8,\n",
        "                \"overlap_comm\": True,\n",
        "                \"reduce_scatter\": True,\n",
        "                \"reduce_bucket_size\": 5e8,\n",
        "                \"contiguous_gradients\": True,\n",
        "                \"cpu_offload\": self.ds_config.get('cpu_offload', False)\n",
        "            },\n",
        "\n",
        "            \"fp16\": {\n",
        "                \"enabled\": True,\n",
        "                \"loss_scale\": 0,\n",
        "                \"loss_scale_window\": 1000,\n",
        "                \"initial_scale_power\": 16,\n",
        "                \"hysteresis\": 2,\n",
        "                \"min_loss_scale\": 1\n",
        "            },\n",
        "\n",
        "            \"optimizer\": {\n",
        "                \"type\": \"AdamW\",\n",
        "                \"params\": {\n",
        "                    \"lr\": self.ds_config['learning_rate'],\n",
        "                    \"betas\": [0.9, 0.999],\n",
        "                    \"eps\": 1e-8,\n",
        "                    \"weight_decay\": 0.01\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"scheduler\": {\n",
        "                \"type\": \"WarmupLR\",\n",
        "                \"params\": {\n",
        "                    \"warmup_min_lr\": 0,\n",
        "                    \"warmup_max_lr\": self.ds_config['learning_rate'],\n",
        "                    \"warmup_num_steps\": 100\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"gradient_clipping\": 1.0,\n",
        "\n",
        "            \"wall_clock_breakdown\": True,\n",
        "\n",
        "            \"memory_breakdown\": True,\n",
        "\n",
        "            \"tensorboard\": {\n",
        "                \"enabled\": True,\n",
        "                \"output_path\": \"./logs/\",\n",
        "                \"job_name\": \"deepspeed_advanced_tutorial\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def initialize_deepspeed(self):\n",
        "        \"\"\"Initialize DeepSpeed engine\"\"\"\n",
        "        print(\"âš¡ Initializing DeepSpeed...\")\n",
        "\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--local_rank', type=int, default=0)\n",
        "        args = parser.parse_args([])\n",
        "\n",
        "        self.engine, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
        "            args=args,\n",
        "            model=self.model,\n",
        "            config=self.create_deepspeed_config()\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ¯ DeepSpeed engine initialized with ZeRO stage {self.ds_config['zero_stage']}\")\n",
        "        return self.engine\n",
        "\n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        \"\"\"Perform a single training step with DeepSpeed optimizations\"\"\"\n",
        "\n",
        "        input_ids = batch['input_ids'].to(self.engine.device)\n",
        "        labels = batch['labels'].to(self.engine.device)\n",
        "\n",
        "        outputs = self.engine(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        self.engine.backward(loss)\n",
        "\n",
        "        self.engine.step()\n",
        "\n",
        "        return {\n",
        "            'loss': loss.item(),\n",
        "            'lr': self.engine.lr_scheduler.get_last_lr()[0] if self.engine.lr_scheduler else 0\n",
        "        }\n",
        "\n",
        "    def train(self, dataloader: DataLoader, num_epochs: int = 2):\n",
        "        \"\"\"Complete training loop with monitoring\"\"\"\n",
        "        print(f\"ğŸ‹ï¸ Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "        self.engine.train()\n",
        "        total_steps = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            epoch_steps = 0\n",
        "\n",
        "            print(f\"\\nğŸ“ˆ Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "            for step, batch in enumerate(dataloader):\n",
        "                start_time = time.time()\n",
        "\n",
        "                metrics = self.train_step(batch)\n",
        "\n",
        "                epoch_loss += metrics['loss']\n",
        "                epoch_steps += 1\n",
        "                total_steps += 1\n",
        "\n",
        "                if step % 10 == 0:\n",
        "                    step_time = time.time() - start_time\n",
        "                    print(f\"  Step {step:4d} | Loss: {metrics['loss']:.4f} | \"\n",
        "                          f\"LR: {metrics['lr']:.2e} | Time: {step_time:.3f}s\")\n",
        "\n",
        "                if step % 20 == 0 and hasattr(self.engine, 'monitor'):\n",
        "                    self.log_memory_stats()\n",
        "\n",
        "                if step >= 50:\n",
        "                    break\n",
        "\n",
        "            avg_loss = epoch_loss / epoch_steps\n",
        "            print(f\"ğŸ“Š Epoch {epoch + 1} completed | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        print(\"ğŸ‰ Training completed!\")\n",
        "\n",
        "    def log_memory_stats(self):\n",
        "        \"\"\"Log GPU memory statistics\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            print(f\"  ğŸ’¾ GPU Memory - Allocated: {allocated:.2f}GB | Reserved: {reserved:.2f}GB\")\n",
        "\n",
        "    def save_checkpoint(self, path: str):\n",
        "        \"\"\"Save model checkpoint using DeepSpeed\"\"\"\n",
        "        print(f\"ğŸ’¾ Saving checkpoint to {path}\")\n",
        "        self.engine.save_checkpoint(path)\n",
        "\n",
        "    def demonstrate_inference(self, text: str = \"The future of AI is\"):\n",
        "        \"\"\"Demonstrate optimized inference with DeepSpeed\"\"\"\n",
        "        print(f\"\\nğŸ”® Running inference with prompt: '{text}'\")\n",
        "\n",
        "        inputs = self.tokenizer.encode(text, return_tensors='pt').to(self.engine.device)\n",
        "\n",
        "        self.engine.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.engine.module.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + 50,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"ğŸ“ Generated text: {generated_text}\")\n",
        "\n",
        "        self.engine.train()"
      ],
      "metadata": {
        "id": "bgB7UjDB-NuU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_advanced_tutorial():\n",
        "    \"\"\"Main function to run the advanced DeepSpeed tutorial\"\"\"\n",
        "\n",
        "    print(\"ğŸŒŸ Advanced DeepSpeed Tutorial Starting...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model_config = {\n",
        "        'vocab_size': 50257,\n",
        "        'seq_length': 512,\n",
        "        'hidden_size': 768,\n",
        "        'num_layers': 6,\n",
        "        'num_heads': 12\n",
        "    }\n",
        "\n",
        "    ds_config = {\n",
        "        'train_batch_size': 16,\n",
        "        'micro_batch_size': 4,\n",
        "        'gradient_accumulation_steps': 4,\n",
        "        'zero_stage': 2,\n",
        "        'learning_rate': 1e-4,\n",
        "        'cpu_offload': False\n",
        "    }\n",
        "\n",
        "    print(\"ğŸ“‹ Configuration:\")\n",
        "    print(f\"  Model size: ~{sum(np.prod(shape) for shape in [[model_config['vocab_size'], model_config['hidden_size']], [model_config['hidden_size'], model_config['hidden_size']] * model_config['num_layers']]) / 1e6:.1f}M parameters\")\n",
        "    print(f\"  ZeRO Stage: {ds_config['zero_stage']}\")\n",
        "    print(f\"  Batch size: {ds_config['train_batch_size']}\")\n",
        "\n",
        "    trainer = AdvancedDeepSpeedTrainer(model_config, ds_config)\n",
        "\n",
        "    model = trainer.create_model()\n",
        "\n",
        "    engine = trainer.initialize_deepspeed()\n",
        "\n",
        "    print(\"\\nğŸ“š Creating synthetic dataset...\")\n",
        "    dataset = SyntheticTextDataset(\n",
        "        size=200,\n",
        "        seq_length=model_config['seq_length'],\n",
        "        vocab_size=model_config['vocab_size']\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=ds_config['micro_batch_size'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nğŸ“Š Pre-training memory stats:\")\n",
        "    trainer.log_memory_stats()\n",
        "\n",
        "    trainer.train(dataloader, num_epochs=2)\n",
        "\n",
        "    print(\"\\nğŸ“Š Post-training memory stats:\")\n",
        "    trainer.log_memory_stats()\n",
        "\n",
        "    trainer.demonstrate_inference(\"DeepSpeed enables efficient training of\")\n",
        "\n",
        "    checkpoint_path = \"./deepspeed_checkpoint\"\n",
        "    trainer.save_checkpoint(checkpoint_path)\n",
        "\n",
        "    demonstrate_zero_stages()\n",
        "    demonstrate_memory_optimization()\n",
        "\n",
        "    print(\"\\nğŸ¯ Tutorial completed successfully!\")\n",
        "    print(\"Key DeepSpeed features demonstrated:\")\n",
        "    print(\"  âœ… ZeRO optimization for memory efficiency\")\n",
        "    print(\"  âœ… Mixed precision training (FP16)\")\n",
        "    print(\"  âœ… Gradient accumulation\")\n",
        "    print(\"  âœ… Learning rate scheduling\")\n",
        "    print(\"  âœ… Checkpoint saving/loading\")\n",
        "    print(\"  âœ… Memory monitoring\")\n",
        "\n",
        "def demonstrate_zero_stages():\n",
        "    \"\"\"Demonstrate different ZeRO optimization stages\"\"\"\n",
        "    print(\"\\nğŸ”§ ZeRO Optimization Stages Explained:\")\n",
        "    print(\"  Stage 0: Disabled (baseline)\")\n",
        "    print(\"  Stage 1: Optimizer state partitioning (~4x memory reduction)\")\n",
        "    print(\"  Stage 2: Gradient partitioning (~8x memory reduction)\")\n",
        "    print(\"  Stage 3: Parameter partitioning (~Nx memory reduction)\")\n",
        "\n",
        "    zero_configs = {\n",
        "        1: {\"stage\": 1, \"reduce_bucket_size\": 5e8},\n",
        "        2: {\"stage\": 2, \"allgather_partitions\": True, \"reduce_scatter\": True},\n",
        "        3: {\"stage\": 3, \"stage3_prefetch_bucket_size\": 5e8, \"stage3_param_persistence_threshold\": 1e6}\n",
        "    }\n",
        "\n",
        "    for stage, config in zero_configs.items():\n",
        "        estimated_memory_reduction = [1, 4, 8, \"Nx\"][stage]\n",
        "        print(f\"  ğŸ“‰ Stage {stage}: ~{estimated_memory_reduction}x memory reduction\")\n",
        "\n",
        "def demonstrate_memory_optimization():\n",
        "    \"\"\"Show memory optimization techniques\"\"\"\n",
        "    print(\"\\nğŸ§  Memory Optimization Techniques:\")\n",
        "    print(\"  ğŸ”„ Gradient Checkpointing: Trade compute for memory\")\n",
        "    print(\"  ğŸ“¤ CPU Offloading: Move optimizer states to CPU\")\n",
        "    print(\"  ğŸ—œï¸ Compression: Reduce communication overhead\")\n",
        "    print(\"  âš¡ Mixed Precision: Use FP16 for faster training\")"
      ],
      "metadata": {
        "id": "6-JjquWB-qW5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepSpeedConfigGenerator:\n",
        "    \"\"\"Utility class to generate DeepSpeed configurations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_config(\n",
        "        batch_size: int = 16,\n",
        "        zero_stage: int = 2,\n",
        "        use_cpu_offload: bool = False,\n",
        "        learning_rate: float = 1e-4\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Generate a complete DeepSpeed configuration\"\"\"\n",
        "\n",
        "        config = {\n",
        "            \"train_batch_size\": batch_size,\n",
        "            \"train_micro_batch_size_per_gpu\": max(1, batch_size // 4),\n",
        "            \"gradient_accumulation_steps\": max(1, batch_size // max(1, batch_size // 4)),\n",
        "\n",
        "            \"zero_optimization\": {\n",
        "                \"stage\": zero_stage,\n",
        "                \"allgather_partitions\": True,\n",
        "                \"allgather_bucket_size\": 5e8,\n",
        "                \"overlap_comm\": True,\n",
        "                \"reduce_scatter\": True,\n",
        "                \"reduce_bucket_size\": 5e8,\n",
        "                \"contiguous_gradients\": True\n",
        "            },\n",
        "\n",
        "            \"fp16\": {\n",
        "                \"enabled\": True,\n",
        "                \"loss_scale\": 0,\n",
        "                \"loss_scale_window\": 1000,\n",
        "                \"initial_scale_power\": 16,\n",
        "                \"hysteresis\": 2,\n",
        "                \"min_loss_scale\": 1\n",
        "            },\n",
        "\n",
        "            \"optimizer\": {\n",
        "                \"type\": \"AdamW\",\n",
        "                \"params\": {\n",
        "                    \"lr\": learning_rate,\n",
        "                    \"betas\": [0.9, 0.999],\n",
        "                    \"eps\": 1e-8,\n",
        "                    \"weight_decay\": 0.01\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"scheduler\": {\n",
        "                \"type\": \"WarmupLR\",\n",
        "                \"params\": {\n",
        "                    \"warmup_min_lr\": 0,\n",
        "                    \"warmup_max_lr\": learning_rate,\n",
        "                    \"warmup_num_steps\": 100\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"gradient_clipping\": 1.0,\n",
        "            \"wall_clock_breakdown\": True\n",
        "        }\n",
        "\n",
        "        if use_cpu_offload:\n",
        "            config[\"zero_optimization\"][\"cpu_offload\"] = True\n",
        "            config[\"zero_optimization\"][\"pin_memory\"] = True\n",
        "\n",
        "        if zero_stage == 3:\n",
        "            config[\"zero_optimization\"].update({\n",
        "                \"stage3_prefetch_bucket_size\": 5e8,\n",
        "                \"stage3_param_persistence_threshold\": 1e6,\n",
        "                \"stage3_gather_16bit_weights_on_model_save\": True\n",
        "            })\n",
        "\n",
        "        return config\n",
        "\n",
        "def benchmark_zero_stages():\n",
        "    \"\"\"Benchmark different ZeRO stages\"\"\"\n",
        "    print(\"\\nğŸ Benchmarking ZeRO Stages...\")\n",
        "\n",
        "    model_config = {\n",
        "        'vocab_size': 50257,\n",
        "        'seq_length': 256,\n",
        "        'hidden_size': 512,\n",
        "        'num_layers': 4,\n",
        "        'num_heads': 8\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for stage in [1, 2]:\n",
        "        print(f\"\\nğŸ”¬ Testing ZeRO Stage {stage}...\")\n",
        "\n",
        "        ds_config = {\n",
        "            'train_batch_size': 8,\n",
        "            'micro_batch_size': 2,\n",
        "            'gradient_accumulation_steps': 4,\n",
        "            'zero_stage': stage,\n",
        "            'learning_rate': 1e-4\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            trainer = AdvancedDeepSpeedTrainer(model_config, ds_config)\n",
        "            model = trainer.create_model()\n",
        "            engine = trainer.initialize_deepspeed()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "                dataset = SyntheticTextDataset(size=20, seq_length=model_config['seq_length'])\n",
        "                dataloader = DataLoader(dataset, batch_size=ds_config['micro_batch_size'])\n",
        "\n",
        "                start_time = time.time()\n",
        "                for i, batch in enumerate(dataloader):\n",
        "                    if i >= 5:\n",
        "                        break\n",
        "                    trainer.train_step(batch)\n",
        "\n",
        "                end_time = time.time()\n",
        "                peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
        "\n",
        "                results[stage] = {\n",
        "                    'peak_memory_gb': peak_memory,\n",
        "                    'time_per_step': (end_time - start_time) / 5\n",
        "                }\n",
        "\n",
        "                print(f\"  ğŸ“Š Peak Memory: {peak_memory:.2f}GB\")\n",
        "                print(f\"  â±ï¸ Time per step: {results[stage]['time_per_step']:.3f}s\")\n",
        "\n",
        "            del trainer, model, engine\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error with stage {stage}: {str(e)}\")\n",
        "\n",
        "    if len(results) > 1:\n",
        "        print(f\"\\nğŸ“ˆ Comparison:\")\n",
        "        stage_1_memory = results.get(1, {}).get('peak_memory_gb', 0)\n",
        "        stage_2_memory = results.get(2, {}).get('peak_memory_gb', 0)\n",
        "\n",
        "        if stage_1_memory > 0 and stage_2_memory > 0:\n",
        "            memory_reduction = (stage_1_memory - stage_2_memory) / stage_1_memory * 100\n",
        "            print(f\"  ğŸ¯ Memory reduction from Stage 1 to 2: {memory_reduction:.1f}%\")\n",
        "\n",
        "def demonstrate_advanced_features():\n",
        "    \"\"\"Demonstrate additional advanced DeepSpeed features\"\"\"\n",
        "    print(\"\\nğŸš€ Advanced DeepSpeed Features:\")\n",
        "\n",
        "    print(\"  ğŸšï¸ Dynamic Loss Scaling: Automatically adjusts FP16 loss scaling\")\n",
        "\n",
        "    print(\"  ğŸ—œï¸ Gradient Compression: Reduces communication overhead\")\n",
        "\n",
        "    print(\"  ğŸ”„ Pipeline Parallelism: Splits model across devices\")\n",
        "\n",
        "    print(\"  ğŸ§‘â€ğŸ“ Expert Parallelism: Efficient Mixture-of-Experts training\")\n",
        "\n",
        "    print(\"  ğŸ“š Curriculum Learning: Progressive training strategies\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"ğŸ–¥ï¸ CUDA Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "\n",
        "    try:\n",
        "        run_advanced_tutorial()\n",
        "\n",
        "        benchmark_zero_stages()\n",
        "\n",
        "        demonstrate_advanced_features()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during tutorial: {str(e)}\")\n",
        "        print(\"ğŸ’¡ Tips for troubleshooting:\")\n",
        "        print(\"  - Ensure you have GPU runtime enabled in Colab\")\n",
        "        print(\"  - Try reducing batch_size or model size if facing memory issues\")\n",
        "        print(\"  - Enable CPU offloading in ds_config if needed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XUFAfN59PQE",
        "outputId": "85fc7583-b6f0-49f0-9376-4011342d9647"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸ CUDA Available: False\n",
            "ğŸŒŸ Advanced DeepSpeed Tutorial Starting...\n",
            "============================================================\n",
            "ğŸ“‹ Configuration:\n",
            "  Model size: ~38.6M parameters\n",
            "  ZeRO Stage: 2\n",
            "  Batch size: 16\n",
            "ğŸ§  Creating model...\n",
            "ğŸ“Š Model parameters: 81,519,360\n",
            "âš¡ Initializing DeepSpeed...\n",
            "[2025-08-31 12:00:33,056] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\n",
            "âŒ Error during tutorial: Type fp16 is not supported on your device.\n",
            "ğŸ’¡ Tips for troubleshooting:\n",
            "  - Ensure you have GPU runtime enabled in Colab\n",
            "  - Try reducing batch_size or model size if facing memory issues\n",
            "  - Enable CPU offloading in ds_config if needed\n"
          ]
        }
      ]
    }
  ]
}